{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a Neural Network using MNIST data\n",
    "\n",
    "In this project we will train a neural network using MNIST data set to predict digit using image as the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn.datasets package contains the MNIST dataset. So let us read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the contents of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COL_NAMES': ['label', 'data'],\n",
       " 'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ..., \n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'target': array([ 0.,  0.,  0., ...,  9.,  9.,  9.])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as a dictionary, the training data are contained in 'data' key, the target labels are contained in 'target' key. The 'COL_NAMES' key has the key names of the dictionary. Let us read the test and training data to X and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dimensions of independent and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has 70000 observations, with 784 columns. So we have 70000 images to train/test our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20849d90780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_digit = X[15000]\n",
    "some_digit_image = some_digit.reshape(28,28)\n",
    "plt.imshow(some_digit_image,cmap=matplotlib.cm.binary,interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test training split**\n",
    "\n",
    "Let us consider the initial 60000 observations for training and the remaining 10000 observations for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:],y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the test and training data has all the digits, with approximate uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    1135\n",
       "2.0    1032\n",
       "7.0    1028\n",
       "3.0    1010\n",
       "9.0    1009\n",
       "4.0     982\n",
       "0.0     980\n",
       "8.0     974\n",
       "6.0     958\n",
       "5.0     892\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    6742\n",
       "7.0    6265\n",
       "3.0    6131\n",
       "2.0    5958\n",
       "9.0    5949\n",
       "0.0    5923\n",
       "6.0    5918\n",
       "8.0    5851\n",
       "4.0    5842\n",
       "5.0    5421\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two displays confirm that our training and test data have all the digits, and these digits have approximately uniform distribution in both the data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform stratified sampling from training data to get the validation data. The validation data will be used for tuning the hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split( X_train, y_train, \\\n",
    "                                                                            test_size=0.1, \\\n",
    "                                                                random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "observations_count = X_train.shape[0]\n",
    "shuffle_index = np.random.permutation(observations_count)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  1.,  3.,  1.,  0.,  0.,  1.,  3.,  6.,  6.,  8.,  4.,  8.,\n",
       "        5.,  5.,  4.,  2.,  7.,  1.,  8.,  9.,  4.,  4.,  9.,  2.,  0.,\n",
       "        2.,  9.,  9.,  3.,  0.,  6.,  2.,  9.,  5.,  9.,  7.,  1.,  2.,\n",
       "        4.,  8.,  5.,  7.,  7.,  6.,  1.,  5.,  2.,  1.,  4.,  3.,  4.,\n",
       "        9.,  3.,  7.,  4.,  1.,  9.,  1.,  8.,  1.,  5.,  3.,  9.,  1.,\n",
       "        9.,  9.,  1.,  4.,  1.,  2.,  3.,  2.,  1.,  2.,  2.,  9.,  1.,\n",
       "        1.,  2.,  9.,  5.,  2.,  4.,  4.,  6.,  1.,  6.,  1.,  7.,  6.,\n",
       "        2.,  8.,  7.,  5.,  4.,  1.,  7.,  0.,  8.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The training data looks unsorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the data for training**\n",
    "\n",
    "Let us standardize the training data and also apply one hot encoding for the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sekhar\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler will standardize the numeric columns\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "X_train_transformed = std_scaler.transform(X_train)\n",
    "\n",
    "#One-hot encoding of target labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(y_train)\n",
    "y_train_transformed = label_binarizer.transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes**\n",
    "\n",
    "Let us check how the target labels are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed\n",
    "label_binarizer.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training neural network**\n",
    "\n",
    "We will use Keras to build and train our neural network. We have the following options to check:\n",
    "\n",
    "* Activation: _relu_, _tanh_, _sigmoid_\n",
    "  * We will first use these three activations with just 2 layers of neural networks. In the first layer we will use 30 nodes and in the last layer we will use 10 (since we have 10 classes)\n",
    "* We will choose the activation function that results in the least validation error\n",
    "\n",
    "**Other parameters**\n",
    "\n",
    "Once the activation function is chosen, we have to choose the remaining parameters, given below:\n",
    "\n",
    "* Number of layers: This can be any number, but we will try 2, 3 layers\n",
    "* Number of nodes in each layer. We will use only 50 epochs with a batch size (for Stochastic Gradient Descent) of 1000.\n",
    "  * If we use only 2 layers, then we will use 128 nodes in the first layer and 10 nodes as the last layer\n",
    "  \n",
    "  * If we use 3 layers then we will use 40 nodes as the first layer, 15 nodes as the second and 10 nodes as the last layer\n",
    "\n",
    "* Dropout: We will use 0.2 as the drop out and use the following number of nodes again. We will use 150 epoches and batch size as 1000 observations. We increased the number of epoches to 150, since we are using dropout of 20%.\n",
    "  * If we use only 2 layers, then we will use 128 nodes in the first layer and 10 nodes as the last layer\n",
    "  \n",
    "  * If we use 3 layers then we will use 128 nodes as the first layer, 40 nodes as the second and 10 nodes as the last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the best optimization function**\n",
    "\n",
    "Let us find which activation function performs better for a 40 node (layer-1) and 10 node (layer-2) network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sekhar\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 35us/step\n",
      "6000/6000 [==============================] - 0s 36us/step\n",
      "6000/6000 [==============================] - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "X_validation_transformed = std_scaler.transform(X_validation)\n",
    "\n",
    "#One-hot encoding of target labels\n",
    "y_validation_transformed = label_binarizer.transform(y_validation)\n",
    "\n",
    "activation = ['relu','sigmoid','tanh']\n",
    "score = dict()\n",
    "\n",
    "layer_1_nodes = 40\n",
    "for i in activation:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(40, input_dim=784))\n",
    "    model.add(Activation(i))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train_transformed, y_train_transformed, epochs=50, batch_size=1000, verbose=0)\n",
    "    score[i] = model.evaluate(X_validation_transformed, y_validation_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation data Accuracy using sigmoid activation is 0.9463333333333334: \n",
      "\n",
      "Validation data Accuracy using relu activation is 0.9645: \n",
      "\n",
      "Validation data Accuracy using tanh activation is 0.9451666666666667: \n"
     ]
    }
   ],
   "source": [
    "for key, value in score.items():\n",
    "    print(\"\\nValidation data Accuracy using {} activation is {}: \".format(key,value[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above display confirms that validation accuracy is greatest for _relu_ activation. Hence we will use _relu_ activation to train our neural network. NOTE that irrespective of the activation functions used in all the layers, the outer layer must use softmax activation, since we have to obtain the probabilities of each of the classes and these probabilities will be used to adjust the weights of the network, using back propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Training options**\n",
    "\n",
    "Given that we selected _relu_ activation, let us work on selecting the other options.\n",
    "\n",
    "We will train the neural network using the following parameters and get the validation score:\n",
    "* Options-1\n",
    "   * epochs=50\n",
    "   * batch_size = 1000\n",
    "   * First layer nodes  = 128\n",
    "   * Last layer nodes = 10\n",
    "   * Activation = 'relu'\n",
    "\n",
    "* Options-2\n",
    "   * epochs=50\n",
    "   * batch_size = 1000\n",
    "   * First layer nodes  = 40\n",
    "   * Second layer nodes = 15\n",
    "   * Last layer nodes = 10\n",
    "   * Activation = 'relu'   \n",
    "\n",
    "* Options-3\n",
    "   * epochs=150\n",
    "   * batch_size = 1000\n",
    "   * First layer nodes  = 128\n",
    "   * Last layer nodes = 10\n",
    "   * Dropout = 0.2\n",
    "   * Activation = 'relu'   \n",
    "   \n",
    "   \n",
    "\n",
    "* Options-4\n",
    "   * epochs=150\n",
    "   * batch_size = 1000\n",
    "   * First layer nodes  = 128\n",
    "   * Second layer nodes = 40\n",
    "   * Last layer nodes = 10\n",
    "   * Activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6000/6000 [==============================] - 0s 54us/step\n"
     ]
    }
   ],
   "source": [
    "score = {}\n",
    "#Options-1\n",
    "#epochs=50\n",
    "#batch_size = 1000\n",
    "#First layer nodes = 128\n",
    "#Last layer nodes = 10\n",
    "#Activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train_transformed, y_train_transformed, epochs=50, batch_size=1000, verbose=0)\n",
    "score['options-1'] = model.evaluate(X_validation_transformed, y_validation_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 40)                31400     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 15)                615       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 32,175\n",
      "Trainable params: 32,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6000/6000 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "#Options-2\n",
    "#epochs=50\n",
    "#batch_size = 1000\n",
    "#First layer nodes = 40\n",
    "#Second layer nodes = 15\n",
    "#Last layer nodes = 10\n",
    "#Activation = 'relu' \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(15))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train_transformed, y_train_transformed, epochs=50, batch_size=1000, verbose=0)\n",
    "score['options-2'] = model.evaluate(X_validation_transformed, y_validation_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6000/6000 [==============================] - 0s 55us/step\n"
     ]
    }
   ],
   "source": [
    "#Options-3\n",
    "#epochs=150\n",
    "#batch_size = 1000\n",
    "#First layer nodes = 128\n",
    "#Last layer nodes = 10\n",
    "#Dropout = 0.2\n",
    "#Activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train_transformed, y_train_transformed, epochs=150, batch_size=1000, verbose=0)\n",
    "score['options-3'] = model.evaluate(X_validation_transformed, y_validation_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 56us/step\n",
      "Test data accuracy  [0.13601595353592738, 0.97740000000000005]\n"
     ]
    }
   ],
   "source": [
    "#Options-4\n",
    "#epochs=150\n",
    "#batch_size = 1000\n",
    "#First layer nodes = 128\n",
    "#Second layer nodes = 40\n",
    "#Last layer nodes = 10\n",
    "#Activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "model.fit(X_train_transformed, y_train_transformed, epochs=150, batch_size=1000, verbose=0)\n",
    "print(\"Test data accuracy \",model.evaluate(X_test_transformed, y_test_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 40)                5160      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 106,050\n",
      "Trainable params: 106,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6000/6000 [==============================] - 0s 62us/step\n"
     ]
    }
   ],
   "source": [
    "#Options-4\n",
    "#epochs=150\n",
    "#batch_size = 1000\n",
    "#First layer nodes = 128\n",
    "#Second layer nodes = 40\n",
    "#Last layer nodes = 10\n",
    "#Activation = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(40))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train_transformed, y_train_transformed, epochs=150, batch_size=1000, verbose=0)\n",
    "score['options-4'] = model.evaluate(X_validation_transformed, y_validation_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the scores of the four options tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options-4\n",
      "Validation accuracy 0.9736666666666667\n",
      "options-1\n",
      "Validation accuracy 0.9711666666666666\n",
      "options-2\n",
      "Validation accuracy 0.9586666666666667\n",
      "options-3\n",
      "Validation accuracy 0.9705\n"
     ]
    }
   ],
   "source": [
    "for key, value in score.items():\n",
    "    print(key)\n",
    "    print(\"Validation accuracy {}\".format(value[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the options listed in option-4 to train our final model, as the validation accuracy is the highest (97.4% approximately).\n",
    "\n",
    "**Training the final neural network**\n",
    "\n",
    "We will use the following options to train our final neural network:\n",
    "\n",
    "* epochs=150\n",
    "* batch_size = 1000\n",
    "* First layer nodes = 128\n",
    "* Second layer nodes = 40\n",
    "* Last layer nodes = 10\n",
    "* Activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sekhar\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:],y[:60000], y[60000:]\n",
    "\n",
    "#Set the random seed \n",
    "np.random.seed(100)\n",
    "\n",
    "#Shffle the training data randomly\n",
    "observations_count = X_train.shape[0]\n",
    "shuffle_index = np.random.permutation(observations_count)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "#Scale the data using std. scaler\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)\n",
    "\n",
    "#One-hot encoding of target labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(y_train)\n",
    "\n",
    "#Use std. scaler for training data\n",
    "X_train_transformed = std_scaler.transform(X_train)\n",
    "y_train_transformed = label_binarizer.transform(y_train)\n",
    "\n",
    "#Use std. scaler for test data\n",
    "X_test_transformed = std_scaler.transform(X_test)\n",
    "y_test_transformed = label_binarizer.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training neural network**\n",
    "\n",
    "We will use Keras to build and train our neural network. We have the following options to check:\n",
    "\n",
    "* Activation: _relu_, _tanh_, _sigmoid_\n",
    "  * We will first use these three activations with just 2 layers of neural networks. In the first layer we will use 30 nodes and in the last layer we will use 10 (since we have 10 classes)\n",
    "* We will choose the activation function that results in the least validation error\n",
    "\n",
    "**Other parameters**\n",
    "\n",
    "Once the activation function is chosen, we have to choose the remaining parameters, given below:\n",
    "\n",
    "* Number of layers: This can be any number, but we will try 2, 3 layers\n",
    "* Number of nodes in each layer. We will use only 50 epochs with a batch size (for Stochastic Gradient Descent) of 1000.\n",
    "  * If we use only 2 layers, then we will use 128 nodes in the first layer and 10 nodes as the last layer\n",
    "  \n",
    "  * If we use 3 layers then we will use 40 nodes as the first layer, 15 nodes as the second and 10 nodes as the last layer\n",
    "\n",
    "* Dropout: We will use 0.2 as the drop out and use the following number of nodes again. We will use 150 epoches and batch size as 1000 observations:\n",
    "  * If we use only 2 layers, then we will use 128 nodes in the first layer and 10 nodes as the last layer\n",
    "  \n",
    "  * If we use 3 layers then we will use 128 nodes as the first layer, 40 nodes as the second and 10 nodes as the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28343996518>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(Dense(128))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(10))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train_transformed, y_train_transformed, epochs=150, batch_size=1000, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 3s 50us/step\n",
      "\n",
      "Accuracy:  0.999933333333\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train_transformed, y_train_transformed)\n",
    "print(\"\\nAccuracy: \", score[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1856/10000 [====>.........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sekhar\\anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 48us/step\n",
      "\n",
      "Accuracy:  0.9762\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed = std_scaler.transform(X_test)\n",
    "\n",
    "#One-hot encoding of target labels\n",
    "y_test_transformed = label_binarizer.transform(y_test)\n",
    "\n",
    "score = model.evaluate(X_test_transformed, y_test_transformed)\n",
    "print(\"\\nAccuracy: \", score[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
